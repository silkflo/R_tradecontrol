---
title: "RL-Simulation--DRAFT"
author: "Vladimir Zhbanko"
date: '2018-08-23'
output:
  html_document:
    df_print: paged
---

# Reinforcement Learning Simulation

Implementing and understanding Generic Example from: [vignette](https://cran.r-project.org/web/packages/ReinforcementLearning/vignettes/ReinforcementLearning.html)

## Goal

This Simulation will attempt to prototype self-regulating system capable to learn which state is more favourable. 

This is inspired by the 6th course of the Lazy Trading Series: 'Detect Market Type using Artificial Intelligence' as well as 7th course 'Self Learning Robot'.

The motivation for this little experiment is coming because of the need to dynamically decide which Market Type will be farourable for specific trading system and which not. The idea of implementation is coming from Reinforcement Learning previously implemented in the 4th course 'Statistical Control of the Trades'.

Plan of this work will be about several building blocks:

* We have Deep Learning model that is classifying which Market Type is currently experienced by the currency pair, known as `State` and `Next State`
* We will have systems of States comprizing of two possibilities 'ON' and 'OFF' - these will correspond to the choices of the system to 'Trade or not to Trade' in the given Market Type
* Generated result of the trade achieved previously by the Trading System also known as `Reward`



## Implementation in R

Code below will help to install package **ReinforcementLearning** if not done yet

```{r}
# package installation
#install.packages("devtools")
#devtools::install_github("nproellochs/ReinforcementLearning")
library(ReinforcementLearning)
library(tidyverse)
```

## Data format

Dataset for this example will be generated in the simple excel file

```{r}
library(readxl)
DF_XL <- read_excel(path = "profit_simulation_understanding.xlsx", sheet = 2)

head(DF_XL, 5)
```


## Description of the environment

In our situation  will need to move our `Robot` to the destination in the cell `s4`. This is how our states will look like:

` |———–--------------------|`
` | s1   s2  s3  s4  s5  s6|`
` |———–--------------------|`

Each state will correspond to our market type periods:

` |———–--------------------|`
` | bun buv bev ben ran rav|`
` |———–--------------------|`

During trading of the system our `Environment` will accumulate knowledge about achieved reward while opening trades in these specific periods

Obvious idea will be that Reinforcement Learning model is going to find the best possible action for each state. Actions we will have will continue to be:

` |———–---|`
` | ON OFF|`
` |———–---|`

We will define our States and Actions using the code below:

```{r}
# Define state and action sets
states <- c("BUN", "BUV", "BEV", "BEN", "RAN", "RAV")
actions <- c("ON", "OFF")
```

RL model should be able to provide direction to our `Robot` and guide him/her to the cell 's4'...

## Dynamics of Environment

In order to generate the Environment data we will be using simple excel table assuming following conditions:

* We generate 20 States for each Market Period
* Training data will have randomly distributed ON/OFF Action across those states 50% ON and 50% OFF
* Reward will tend to be positive for Normal Markets and negative for Volatile Market (pure example for demo purposes)
* Next State will be random

## Reading simulated data

We read our data:

```{r}
# reading data 
DF_XL <- read_excel(path = "profit_simulation_understanding.xlsx", sheet = 2) %>% as.data.frame()
# make a little summary
DF_XL %>% mutate_at(c(1,2,4), as.factor) %>% summary(DF_XL)
```

This is of course just to start making progress, later we can make more funny data to try with...

## Reinforcement learning parameters

As we already know there are 3 parameters defining how the system will learn. We will define them using this list

```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.8, gamma = 0.5, epsilon = 0.1)
```

## Reinforcement learning

Performing RL is very quick using a code below:

```{r}
# Perform reinforcement learning
model <- ReinforcementLearning(DF_XL, s = "State", a = "Action", r = "Reward", 
                               s_new = "NextState", control = control)
```
 
Model is ready. Before we explore it let's perform a little analysis of our data by ordering data to groups and summarising it:

```{r}
DF_XL %>% group_by(State) %>% summarize(sum = sum(Reward)) 
```

We can now confirm that System is 'profitable' during 'Normal' market and 'not profitable' during 'Volatile'

Curious what our model is saying:


```{r}
model$Policy
```



```{r}
# Print result
print(model)
```


## Reinforcement Learning Policy

The RL result can be extracted by using the policy function:

```{r}
policy(model)
```

## How to use RL Policy?

Policy of the model can be used to select best action based on the current state. For example:

```{r}
next_action <- policy(model)["BUN"]
next_action
```

```{r}
next_action <- policy(model)["BUV"]
next_action
```


## Updating Existing Policy with New data

We will now create a new dataset where data will be shifted

```{r}
# reading data 
DF_XL1 <- read_excel(path = "profit_simulation_understanding.xlsx", sheet = 3) %>% as.data.frame()
# make a little summary
DF_XL1 %>% group_by(State) %>% summarize(sum = sum(Reward)) 
```

```{r}
# Update the existing policy using new training data
model_new <- ReinforcementLearning(DF_XL1, s = "State", a = "Action", r = "Reward", 
                                   s_new = "NextState", control = control,iter = 20, model = model)


```

## New model

```{r}

# Print result
print(model_new)
```

Now the reward is much higher

```{r}
summary(model_new)
```

```{r}
plot(model_new)
```

## Conclusion

We had fully simulated Environment (very boring!) however now we know how:

- data should look like
- how to train model
- how to use policy or how to derive best action by knowing current state

This knowledge can be used to simulate completely different environment...

## Parameters explained

**"alpha"** The learning rate, set between 0 and 1. Setting it to 0 means that the Q-values are never updated and, hence, nothing is learned. Setting a high value, such as 0.9, means that learning can occur quickly.

**"gamma"** Discount factor, set between 0 and 1. Determines the importance of future rewards. A factor of 0 will render the agent short-sighted by only considering current rewards, while a factor approaching 1 will cause it to strive for a greater reward over the long term.

**"epsilon"** Exploration parameter, set between 0 and 1. Defines the exploration mechanism in ϵϵ-greedy action selection. In this strategy, the agent explores the environment by selecting an action at random with probability ϵϵ. Alternatively, the agent exploits its current knowledge by choosing the optimal action with probability 1−ϵ1−ϵ. This parameter is only required for sampling new experience based on an existing policy.

**"iter"** Number of repeated learning iterations. Iter is an integer greater than 0. The default is set to 1.

